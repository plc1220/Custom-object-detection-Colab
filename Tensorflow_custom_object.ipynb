{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow-custom-object.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpiBhE8jIhTh"
      },
      "source": [
        "Mount google drive\r\n",
        "\r\n",
        "Upload the tensorflow folder with correct directory to google drive\r\n",
        "\r\n",
        "Similar to this reference link\r\n",
        "\r\n",
        "https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html\r\n",
        "\r\n",
        "Remember to change your runtime to GPU for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txsvZDQN_oHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2010950d-43b7-474b-a2b9-c30bfdaa891f"
      },
      "source": [
        "# Step 1 - Mount Gdrive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QXze2TiJH5i"
      },
      "source": [
        "Install all the required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYqCodbnDY7M"
      },
      "source": [
        "# Step 2 - Install dependencies\r\n",
        "!apt-get install protobuf-compiler python-lxml python-pil\r\n",
        "!pip install Cython pandas tf-slim lvis\r\n",
        "!pip install avro-python3==1.8.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UikfuShJ_5s"
      },
      "source": [
        "Protobuf Installation/Compilation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBvu6y6qDaor",
        "outputId": "72c457aa-b0a1-4fe1-deac-ce47eb916220"
      },
      "source": [
        "#Step 3 - Protobuf Installation\r\n",
        "#cd into 'TensorFlow/models/research'\r\n",
        "%cd '/content/gdrive/My Drive/Tensorflow/models/research/'\r\n",
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Tensorflow/models/research\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h84Y88dIKNeH"
      },
      "source": [
        "Since using a VM, have to specific the path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SmEtijYDazt"
      },
      "source": [
        "#Step 4- Set the environment.\r\n",
        "\r\n",
        "import os\r\n",
        "import sys\r\n",
        "os.environ['PYTHONPATH']+=\":/content/gdrive/My Drive/Tensorflow/models\"\r\n",
        "sys.path.append(\"/content/gdrive/My Drive/Tensorflow/models/research\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAgkVkTQDa9I"
      },
      "source": [
        "#Step 5- Build and Install setup.py.\r\n",
        "\r\n",
        "!python setup.py build\r\n",
        "!python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiZ2DjrOKo4X"
      },
      "source": [
        "Navigate to dedicated project, then start the tensorboard for training evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQecin8fKnuF"
      },
      "source": [
        "#Step 6 - Tensorboard\r\n",
        "%cd '/content/gdrive/My Drive/Tensorflow/workspace/Traffic_cone/'\r\n",
        "\r\n",
        "#start the Tensorboard\r\n",
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir=models/my_ssd_resnet50_v1_fpn\r\n",
        "\r\n",
        "# %load_ext tensorboard\r\n",
        "# %tensorboard --logdir=models/[name_of_pre-trained-model_you_downloaded]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlmnQ5QKLyVh"
      },
      "source": [
        "Change the pipeline.config parameters to yield optimum training result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_3FnfWWC9EZ"
      },
      "source": [
        "# Step 7 - Training\r\n",
        "!python model_main_tf2.py --model_dir=models/my_ssd_resnet50_v1_fpn --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfyWNOx6ZuuO",
        "outputId": "80001ea5-c04c-4da9-fafe-1c40b1a5b38c"
      },
      "source": [
        "#Step 8- Test the Model.\r\n",
        "\r\n",
        "#Loading the saved_model\r\n",
        "import tensorflow as tf\r\n",
        "import time\r\n",
        "from object_detection.utils import label_map_util\r\n",
        "from object_detection.utils import visualization_utils as viz_utils\r\n",
        "\r\n",
        "PATH_TO_SAVED_MODEL=\"/content/gdrive/My Drive/Tensorflow/workspace/Traffic_cone/exported-models/my_model/saved_model\"\r\n",
        "\r\n",
        "print('Loading model...', end='')\r\n",
        "\r\n",
        "# Load saved model and build the detection function\r\n",
        "detect_fn=tf.saved_model.load(PATH_TO_SAVED_MODEL)\r\n",
        "\r\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model...Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYZ1R8_2TxvZ"
      },
      "source": [
        "#Step 9- Testing the Model.\r\n",
        "from object_detection.utils import label_map_util\r\n",
        "\r\n",
        "#Loading the label_map\r\n",
        "category_index=label_map_util.create_category_index_from_labelmap(\"/content/gdrive/My Drive/Tensorflow/workspace/Traffic_cone/annotations/label_map.pbtxt\",use_display_name=True)\r\n",
        "\r\n",
        "#category_index=label_map_util.create_category_index_from_labelmap([path_to_label_map],use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhIXng1kUTvm",
        "outputId": "b240b12f-9639-43f9-a63a-f978700e7b89"
      },
      "source": [
        "#Step 10- Testing the Model.\r\n",
        "\r\n",
        "#Loading the image\r\n",
        "img=['/content/gdrive/My Drive/Tensorflow/workspace/Traffic_cone/images/train/1.jpg','/content/gdrive/My Drive/Tensorflow/workspace/Traffic_cone/images/train/500.jpg']\r\n",
        "print(img)\r\n",
        "\r\n",
        "#list containing paths of all the images"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/gdrive/My Drive/Tensorflow/workspace/Traffic_cone/images/train/1.jpg', '/content/gdrive/My Drive/Tensorflow/workspace/Traffic_cone/images/train/500.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plwjtKA3Y-WJ"
      },
      "source": [
        "#Step 11- Running the Inference.\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from PIL import Image\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\r\n",
        "\r\n",
        "def load_image_into_numpy_array(path):\r\n",
        "    \"\"\"Load an image from file into a numpy array.\r\n",
        "\r\n",
        "    Puts image into numpy array to feed into tensorflow graph.\r\n",
        "    Note that by convention we put it into a numpy array with shape\r\n",
        "    (height, width, channels), where channels=3 for RGB.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      path: the file path to the image\r\n",
        "\r\n",
        "    Returns:\r\n",
        "      uint8 numpy array with shape (img_height, img_width, 3)\r\n",
        "    \"\"\"\r\n",
        "    return np.array(Image.open(path))\r\n",
        "\r\n",
        "for image_path in img:\r\n",
        "\r\n",
        "    print('Running inference for {}... '.format(image_path), end='')\r\n",
        "    image_np=load_image_into_numpy_array(image_path)\r\n",
        "\r\n",
        "    # Things to try:\r\n",
        "    # Flip horizontally\r\n",
        "    # image_np = np.fliplr(image_np).copy()\r\n",
        "    # Convert image to grayscale\r\n",
        "    # image_np = np.tile(\r\n",
        "    #     np.mean(image_np, 2, keepdims=True), (1, 1, 3)).astype(np.uint8)\r\n",
        "\r\n",
        "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\r\n",
        "    input_tensor=tf.convert_to_tensor(image_np)\r\n",
        "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\r\n",
        "    input_tensor=input_tensor[tf.newaxis, ...]\r\n",
        "\r\n",
        "    # input_tensor = np.expand_dims(image_np, 0)\r\n",
        "    detections=detect_fn(input_tensor)\r\n",
        "\r\n",
        "    # All outputs are batches tensors.\r\n",
        "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\r\n",
        "    # We're only interested in the first num_detections.\r\n",
        "    num_detections=int(detections.pop('num_detections'))\r\n",
        "    detections={key:value[0,:num_detections].numpy()\r\n",
        "                   for key,value in detections.items()}\r\n",
        "    detections['num_detections']=num_detections\r\n",
        "\r\n",
        "    # detection_classes should be ints.\r\n",
        "    detections['detection_classes']=detections['detection_classes'].astype(np.int64)\r\n",
        "\r\n",
        "    image_np_with_detections=image_np.copy()\r\n",
        "\r\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\r\n",
        "          image_np_with_detections,\r\n",
        "          detections['detection_boxes'],\r\n",
        "          detections['detection_classes'],\r\n",
        "          detections['detection_scores'],\r\n",
        "          category_index,\r\n",
        "          use_normalized_coordinates=True,\r\n",
        "          max_boxes_to_draw=1,     #max number of bounding boxes in the image\r\n",
        "          min_score_thresh=.3,      #min prediction threshold\r\n",
        "          agnostic_mode=False)\r\n",
        "    %matplotlib inline\r\n",
        "    plt.figure()\r\n",
        "    plt.imshow(image_np_with_detections)\r\n",
        "    print('Done')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unj_gUNhMHZZ"
      },
      "source": [
        "Once model is trained, it can be export to deploy on other machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF4bLyZ20EAm"
      },
      "source": [
        "# Step 12 - Export Model\r\n",
        "!python exporter_main_v2.py --input_type image_tensor --pipeline_config_path=models/my_ssd_resnet50_v1_fpn/pipeline.config --trained_checkpoint_dir=models/my_ssd_resnet50_v1_fpn/ --output_directory=exported-models/my_model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}